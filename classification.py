# -*- coding: utf-8 -*-
"""main_classifier.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16XIiSsfC8qsEi04jm2MUOvC9I-NYb9kk
"""

#from google.colab import drive
#drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
import os
import sys, getopt
import keras
from matplotlib import pyplot as plt
import numpy as np
import gzip
import tensorflow as tf
# %matplotlib inline
from keras.models import Model
from keras.optimizers import RMSprop
from keras.layers import Input,Dense,Flatten,Dropout,merge,Reshape,Conv2D,MaxPooling2D,UpSampling2D,Conv2DTranspose
from keras.layers.normalization import BatchNormalization
from keras.models import Model,Sequential
from keras.callbacks import ModelCheckpoint
from keras.optimizers import Adadelta, RMSprop,SGD,Adam
from keras import regularizers
from keras import backend as K
from keras.utils import to_categorical
import struct
from array import array
from sklearn.model_selection import train_test_split

class MnistDataloader(object):
    def __init__(self, training_images_filepath,training_labels_filepath,
                 test_images_filepath, test_labels_filepath):
        self.training_images_filepath = training_images_filepath
        self.training_labels_filepath = training_labels_filepath
        self.test_images_filepath = test_images_filepath
        self.test_labels_filepath = test_labels_filepath
  
    def read_images_labels(self, images_filepath, labels_filepath):        
        labels = []
        with open(labels_filepath, 'rb') as file:
            magic, size = struct.unpack(">II", file.read(8))
            if magic != 2049:
                raise ValueError('Magic number mismatch, expected 2049, got {}'.format(magic))
            labels = array("B", file.read())        
        
        with open(images_filepath, 'rb') as file:
            magic, size, rows, cols = struct.unpack(">IIII", file.read(16))
            if magic != 2051:
                raise ValueError('Magic number mismatch, expected 2051, got {}'.format(magic))
            image_data = array("B", file.read())        
        images = []
        for i in range(size):
            images.append([0] * rows * cols)
        for i in range(size):
            img = np.array(image_data[i * rows * cols:(i + 1) * rows * cols])
            img = img.reshape(28, 28)
            images[i][:] = img            
        
        return images, labels
            
    def load_data(self):
        train_x, train_y = self.read_images_labels(self.training_images_filepath, self.training_labels_filepath)
        test_x, test_y = self.read_images_labels(self.test_images_filepath, self.test_labels_filepath)
        return (train_x, train_y),(test_x, test_y)

def encoder(input_img, num_conv2d, conv2d_sizes): 

    if (num_conv2d < 1):
        print("Number of convolutional layers must be at least 1.")
        return
    elif (num_conv2d != len(conv2d_sizes)):
        print("Number of convolutional layers and length of convolutional layer sizes list must agree.")
        return 
    else:
        encode = Conv2D(conv2d_sizes[0], (3, 3), activation='sigmoid', padding='same')(input_img)
        encode = BatchNormalization()(encode)
        encode = MaxPooling2D(pool_size=(2, 2))(encode)
        encode = Dropout(0.25)(encode)
        for layer in range(1,num_conv2d):
            encode = Conv2D(conv2d_sizes[layer], (3, 3), activation='sigmoid', padding='same')(encode)
            encode = BatchNormalization()(encode)
            if (layer == 1): 
                encode = MaxPooling2D(pool_size=(2, 2))(encode)
                encode = Dropout(0.25)(encode)
            if (layer != num_conv2d - 1 and layer != 1):
                encode = Dropout(0.30)(encode)
        return encode


def decoder(encode, num_conv2d, conv2d_sizes):    

    if (num_conv2d < 1):
        print("Number of convolutional layers must be at least 1.")
        return
    elif (num_conv2d != len(conv2d_sizes)):
        print("Number of convolutional layers and length of convolutional layer sizes list must agree.")
        return 
    else:
        sizes = list()
        sizes = conv2d_sizes
        sizes.pop()
        sizes.reverse()
        decode = Conv2D(sizes[0], (3, 3), activation='sigmoid', padding='same')(encode)
        decode = BatchNormalization()(decode)
        for layer in range(1,num_conv2d - 1):
            if (layer >= num_conv2d - 3):
                decode = UpSampling2D((2,2))(decode)
            decode = Conv2D(sizes[layer], (3, 3), activation='sigmoid', padding='same')(decode)
            if (layer != num_conv2d - 2):
                decode = BatchNormalization()(decode)
        return decode

def fc(encode,batch_size,num_classes):
    flat = Flatten()(encode)
    den = Dense(batch_size, activation='sigmoid')(flat)
    out = Dense(num_classes, activation='softmax')(den)
    return out

def main(argv):

    #DATA LOADING

    #training_images_filepath = '/content/drive/My Drive/train-images-idx3-ubyte'
    #training_labels_filepath = '/content/drive/My Drive/train-labels-idx1-ubyte'
    #test_images_filepath = '/content/drive/My Drive/t10k-images-idx3-ubyte'
    #test_labels_filepath = '/content/drive/My Drive/t10k-labels-idx1-ubyte'
    #trained_model = '/content/drive/MyDrive/Colab Notebooks/autoencoder.h5'
    training_images_filepath = ''
    training_labels_filepath = ''
    test_images_filepath = ''
    test_labels_filepath = ''
    trained_model = ''

    argc =  len(sys.argv)
    if argc != 11:
        print("Error getting arguments from command line!")
        sys.exit=()
    for i in range(1, argc):
        if sys.argv[i] == "-d":
            training_images_filepath = argv[i + 1]
        elif sys.argv[i] == "-dl":
            training_labels_filepath = argv[i + 1]
        elif sys.argv[i] == "-t":
            test_images_filepath = argv[i + 1]
        elif sys.argv[i] == "-tl":
            test_labels_filepath = argv[i + 1]
        elif sys.argv[i] == "-model":
            trained_model = argv[i + 1]

    mnist_dataloader = MnistDataloader(training_images_filepath, training_labels_filepath, test_images_filepath, test_labels_filepath)
    (training_data, training_labels), (testing_data, testing_labels) = mnist_dataloader.load_data()

    #DATA PREPROCESSING

    #convert data into numpy array
    training_data = np.array(training_data)
    testing_data = np.array(testing_data)

    #specify datatype
    traintraining_data_x = training_data.astype('float32')
    testing_data = testing_data.astype('float32')

    #convert each 28 x 28 image into a matrix of size 28 x 28 x 1, which will be fed into the network
    training_data = training_data.reshape(len(training_data), 28, 28, 1)
    testing_data = testing_data.reshape(len(testing_data), 28, 28, 1)

    #rescale  with respective maximum pixel value 
    training_data = training_data / 255.0
    testing_data = testing_data / 255.0
    
    # divide the data properly (80% train , 20% validation)
    train_x,valid_x,train_y,valid_y = train_test_split(training_data, training_data, test_size=0.2, random_state=13)
    
    history_dict = dict()
    count = 0

    while(1):

        #request hyperparameters from the user
        batch_size = int(input("Type batch size: "))
        epochs = int(input("Type epochs: "))
        num_conv2d = int(input("Type number of convolutional layers: "))
        conv2d_sizes = list()
        for i in range(num_conv2d):
            conv2d_sizes.append(int(input("Type size of convolutional layer: ")))
        inChannel = 1
        x, y = 28, 28
        input_img = Input(shape = (x, y, inChannel))
        num_classes = 10
        conv2d_sizes_copy = tuple(conv2d_sizes)

        #create model
        autoencoder = Model(input_img, decoder(encoder(input_img, num_conv2d, conv2d_sizes), num_conv2d, conv2d_sizes))

        #compile model using the optimizer to be RMSProp
        autoencoder.compile(loss='mean_squared_error', optimizer = RMSprop())


        autoencoder.summary() #optional visualize the layers created above

        #model training
        autoencoder_train = autoencoder.fit(train_x, train_y, batch_size=batch_size,epochs=epochs,verbose=1,validation_data=(valid_x, valid_y))
        autoencoder.save_weights('autoencoder.h5')

        training_y_one_hot = to_categorical(training_labels)
        testing_y_one_hot = to_categorical(testing_labels)

        train_x,valid_x,train_label,valid_label = train_test_split(training_data, training_y_one_hot, test_size=0.2, random_state=13)
        conv2d_sizes = list(conv2d_sizes_copy)
        encode = encoder(input_img, num_conv2d, conv2d_sizes)
        full_model = Model(input_img,fc(encode, batch_size,num_classes))

        for l1,l2 in zip(full_model.layers[:(3*num_conv2d + 1)],autoencoder.layers[0:(3*num_conv2d + 1)]):
            l1.set_weights(l2.get_weights())

        for layer in full_model.layers[0:(3*num_conv2d + 1)]:
            layer.trainable = False

        full_model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adam(),metrics=['accuracy'])
        full_model.summary()

        classify_train = full_model.fit(train_x, train_label, batch_size= batch_size,epochs=epochs,verbose=1,validation_data=(valid_x, valid_label))

        full_model.save_weights('autoencoder_classification.h5')

        for layer in full_model.layers[0:(3*num_conv2d + 1)]:
            layer.trainable = True

        accuracy = classify_train.history['accuracy']
        val_accuracy = classify_train.history['val_accuracy']
        loss = classify_train.history['loss']
        val_loss = classify_train.history['val_loss']

        test_eval = full_model.evaluate(testing_data, testing_y_one_hot, verbose=0)
        predicted_classes = full_model.predict(testing_data)
        predicted_classes = np.argmax(np.round(predicted_classes),axis=1)
        from sklearn.metrics import classification_report
        target_names = ["Class {}".format(i) for i in range(num_classes)]
        
        history_dict[count] = (epochs,batch_size,num_conv2d, accuracy, val_accuracy, loss, val_loss, test_eval[0], test_eval[1], classification_report(np.array(testing_labels), predicted_classes, target_names=target_names))
        print(history_dict)

        print("Type 1 to continue with another experiment.")
        print("Type 2 to to show plots and classification report.")
        print("Type 3 to continue with classification of validation set.")
        try:
            choice = int(input())
            if(choice == 1):
                count += 1
                continue
            elif(choice == 2):
    
                for x in range(count + 1):
                    plt.plot(range(history_dict[x][0]),history_dict[x][3] , 'bo', label='Training accuracy')
                    plt.plot(range(history_dict[x][0]), history_dict[x][4], 'b', label='Validation accuracy')
                    plt.title('Training and validation accuracy')
                    plt.legend()
                    plt.figure()
                    plt.show()
                    plt.plot(range(history_dict[x][0]),history_dict[x][5], 'bo', label='Training loss')
                    plt.plot(range(history_dict[x][0]),history_dict[x][6], 'b', label='Validation loss')
                    plt.title('Training and validation loss')
                    plt.legend()
                    plt.figure()
                    plt.show() 
                    print('Test loss:', history_dict[x][7])
                    print('Test accuracy:', history_dict[x][8])
                    print('classification report')
                    print(history_dict[x][9])
                break

            elif(choice == 3):
                full_model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adam(),metrics=['accuracy'])
                classify_train = full_model.fit(train_x, train_label, batch_size=batch_size,epochs=epochs,verbose=1,validation_data=(valid_x, valid_label))
                    
                    
                predicted_classes = full_model.predict(testing_data)
                predicted_classes = np.argmax(np.round(predicted_classes),axis=1)

                correct = np.where(predicted_classes==testing_labels)[0]
                print ("Found %d correct labels" % len(correct))
                for i, correct in enumerate(correct[:9]):
                    plt.subplot(3,3,i+1)
                    plt.imshow(testing_data[correct].reshape(28,28), cmap='gray', interpolation='none')
                    plt.title("Predicted {}, Class {}".format(predicted_classes[correct], testing_labels[correct]))
                    plt.tight_layout()

                incorrect = np.where(predicted_classes!=testing_labels)[0]
                print ("Found %d incorrect labels" % len(incorrect))
                for i, incorrect in enumerate(incorrect[:9]):
                    plt.subplot(3,3,i+1)
                    plt.imshow(testing_data[incorrect].reshape(28,28), cmap='gray', interpolation='none')
                    plt.title("Predicted {}, Class {}".format(predicted_classes[incorrect], testing_labels[incorrect]))
                    plt.tight_layout()
                    break
            else:
                raise Exception("Invalid key.")
        except:
            print("Program terminates.")
            break

if __name__ == "__main__":
    main(sys.argv[:])